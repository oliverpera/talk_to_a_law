{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# REPLICATE_API_TOKEN = os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "TEXT_FILES_FOLDER = \"../resources/Wissensquellen\"\n",
    "SPLITS_PATH = \"../resources/splits/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core import documents\n",
    "import pandas as pd\n",
    "\n",
    "def create_document(chunk):\n",
    "    page_content = chunk\n",
    "    # metadata = {\n",
    "    #     \"splitter\": ,\n",
    "    #     \"\",}\n",
    "    doc = documents.Document(\n",
    "        page_content=page_content,\n",
    "        # metadata=metadata,\n",
    "    )\n",
    "    return doc\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from replicate import Client\n",
    "import spacy\n",
    "\n",
    "# USABLE BUT DEPENDING ON HF IT TAKES VERY LONG TO GET EMBEDDINGS\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=HUGGINGFACE_API_KEY,\n",
    "    # model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # das ist der default\n",
    ")\n",
    " \n",
    "replicateSession = Client(api_token=\"r8_c7xatuN0lmkBFqlgCXvas56nRkRWBeS1aXtWL\")\n",
    "class ReplicateEmbeddingsFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        embeddings = [replicateSession.run(\n",
    "            \"replicate/all-mpnet-base-v2:b6b7585c9640cd7a9572c6e129c9549d79c9c31f0d3fdce7baac7c67ca38f305\",\n",
    "            input={\"text\": document},\n",
    "        )[0]['embedding'] for document in input]\n",
    "        return embeddings\n",
    "replicate_ef = ReplicateEmbeddingsFunction()\n",
    "\n",
    "class SpacyEmbeddingsFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        nlp = spacy.load(\"de_core_news_lg\") # md , lg\n",
    "        embeddings = [nlp(document).vector.tolist() for document in input]\n",
    "        return embeddings\n",
    "spacy_ef = SpacyEmbeddingsFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chroma Client and Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../resources/chromadb\")\n",
    "\n",
    "def create_collection(split_folder: str, embedding_function: EmbeddingFunction, ef_name: str):\n",
    "    file_names = os.listdir(SPLITS_PATH + split_folder)\n",
    "\n",
    "    collection_name = split_folder[:-1] + \"_\" + ef_name\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        # metadata={\"hnsw:space\": \"cosine\"} # \"l2\" (default: squared L2 norm), \"ip\" or \"cosine\"\n",
    "        )\n",
    "\n",
    "    for file in file_names:\n",
    "        file_path = SPLITS_PATH + split_folder + file\n",
    "        df = pd.read_json(file_path)\n",
    "        documents_ = df['splits'].to_list()\n",
    "        ids = [file[:-4] + \"#\" + str(i) for i in range(0, len(documents_))]\n",
    "        metadata = [{\"file\": file[:-4]} for i in range(0, len(documents_))] # doesn`t work because of filename\n",
    "        if len(documents_) == 0:\n",
    "            print(r'This document {filepath} has no splits. It won`t be added to the collection')\n",
    "            break\n",
    "        number_documents = len(documents_)\n",
    "        batchsize = 1000 # need to add in batches as batchsize cap is 5461 \n",
    "        batch_indexes = [i * batchsize for i in range (1, number_documents//batchsize + 1)]\n",
    "        batch_indexes.append(number_documents)\n",
    "        current = 0\n",
    "        print(f\"Adding {len(documents_)} Chunks from {file} to collection {collection.name} in {len(batch_indexes)} batches.\")\n",
    "        for index in batch_indexes:\n",
    "            documents_batch = documents_[current:index]\n",
    "            ids_batch = ids[current:index]\n",
    "            metadata_batch = metadata[current:index]\n",
    "            collection.add(\n",
    "                ids=ids_batch,\n",
    "                documents=documents_batch,\n",
    "                metadatas=metadata_batch\n",
    "                )\n",
    "            print(f\"    Batch '{current} - {index}' complete\")\n",
    "            current = index\n",
    "\n",
    "        print(f\"Success - length of collection {collection.name} is {collection.count()}\")\n",
    "\n",
    "split_folder = SPLITS_PATH + \"char_splitter_128_o0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 532 Chunks from CELEX_02013L0036-20220101_DE_TXT.json to collection char_splitter_1024_o128_spacy in 1 batches.\n",
      "    Batch '0 - 532' complete\n",
      "Success - length of collection char_splitter_1024_o128_spacy is 532\n",
      "Adding 2292 Chunks from CELEX_02013R0575-20230628_DE_TXT.json to collection char_splitter_1024_o128_spacy in 3 batches.\n",
      "    Batch '0 - 1000' complete\n",
      "    Batch '1000 - 2000' complete\n",
      "    Batch '2000 - 2292' complete\n",
      "Success - length of collection char_splitter_1024_o128_spacy is 2824\n",
      "Adding 988 Chunks from KWG.json to collection char_splitter_1024_o128_spacy in 1 batches.\n",
      "    Batch '0 - 988' complete\n",
      "Success - length of collection char_splitter_1024_o128_spacy is 3812\n",
      "Adding 171 Chunks from CELEX_02013L0036-20220101_DE_TXT.json to collection article_regex_splitter_spacy in 1 batches.\n",
      "    Batch '0 - 171' complete\n",
      "Success - length of collection article_regex_splitter_spacy is 171\n",
      "Adding 549 Chunks from CELEX_02013R0575-20230628_DE_TXT.json to collection article_regex_splitter_spacy in 1 batches.\n",
      "    Batch '0 - 549' complete\n",
      "Success - length of collection article_regex_splitter_spacy is 720\n",
      "Adding 6793 Chunks from KWG.json to collection article_regex_splitter_spacy in 7 batches.\n",
      "    Batch '0 - 1000' complete\n",
      "    Batch '1000 - 2000' complete\n",
      "    Batch '2000 - 3000' complete\n",
      "    Batch '3000 - 4000' complete\n",
      "    Batch '4000 - 5000' complete\n",
      "    Batch '5000 - 6000' complete\n",
      "    Batch '6000 - 6793' complete\n",
      "Success - length of collection article_regex_splitter_spacy is 7513\n",
      "Adding 95 Chunks from CELEX_02013L0036-20220101_DE_TXT.json to collection semantic_splitter_spacy in 1 batches.\n",
      "    Batch '0 - 95' complete\n",
      "Success - length of collection semantic_splitter_spacy is 95\n",
      "Adding 273 Chunks from CELEX_02013R0575-20230628_DE_TXT.json to collection semantic_splitter_spacy in 1 batches.\n",
      "    Batch '0 - 273' complete\n",
      "Success - length of collection semantic_splitter_spacy is 368\n",
      "Adding 331 Chunks from KWG.json to collection semantic_splitter_spacy in 1 batches.\n",
      "    Batch '0 - 331' complete\n",
      "Success - length of collection semantic_splitter_spacy is 699\n",
      "Adding 532 Chunks from CELEX_02013L0036-20220101_DE_TXT.json to collection char_splitter_1024_o128_replicate in 1 batches.\n",
      "    Batch '0 - 532' complete\n",
      "Success - length of collection char_splitter_1024_o128_replicate is 532\n",
      "Adding 2292 Chunks from CELEX_02013R0575-20230628_DE_TXT.json to collection char_splitter_1024_o128_replicate in 3 batches.\n",
      "    Batch '0 - 1000' complete\n",
      "    Batch '1000 - 2000' complete\n",
      "    Batch '2000 - 2292' complete\n",
      "Success - length of collection char_splitter_1024_o128_replicate is 2824\n",
      "Adding 988 Chunks from KWG.json to collection char_splitter_1024_o128_replicate in 1 batches.\n",
      "    Batch '0 - 988' complete\n",
      "Success - length of collection char_splitter_1024_o128_replicate is 3812\n",
      "Adding 171 Chunks from CELEX_02013L0036-20220101_DE_TXT.json to collection article_regex_splitter_replicate in 1 batches.\n",
      "    Batch '0 - 171' complete\n",
      "Success - length of collection article_regex_splitter_replicate is 171\n",
      "Adding 549 Chunks from CELEX_02013R0575-20230628_DE_TXT.json to collection article_regex_splitter_replicate in 1 batches.\n",
      "    Batch '0 - 549' complete\n",
      "Success - length of collection article_regex_splitter_replicate is 720\n",
      "Adding 6793 Chunks from KWG.json to collection article_regex_splitter_replicate in 7 batches.\n",
      "    Batch '0 - 1000' complete\n",
      "    Batch '1000 - 2000' complete\n",
      "    Batch '2000 - 3000' complete\n",
      "    Batch '3000 - 4000' complete\n",
      "    Batch '4000 - 5000' complete\n",
      "    Batch '5000 - 6000' complete\n",
      "    Batch '6000 - 6793' complete\n",
      "Success - length of collection article_regex_splitter_replicate is 7513\n",
      "Adding 95 Chunks from CELEX_02013L0036-20220101_DE_TXT.json to collection semantic_splitter_replicate in 1 batches.\n",
      "    Batch '0 - 95' complete\n",
      "Success - length of collection semantic_splitter_replicate is 95\n",
      "Adding 273 Chunks from CELEX_02013R0575-20230628_DE_TXT.json to collection semantic_splitter_replicate in 1 batches.\n",
      "    Batch '0 - 273' complete\n",
      "Success - length of collection semantic_splitter_replicate is 368\n",
      "Adding 331 Chunks from KWG.json to collection semantic_splitter_replicate in 1 batches.\n",
      "    Batch '0 - 331' complete\n",
      "Success - length of collection semantic_splitter_replicate is 699\n"
     ]
    }
   ],
   "source": [
    "create_collection(\"char_splitter_1024_o128/\", spacy_ef, \"spacy\")\n",
    "create_collection(\"article_regex_splitter/\", spacy_ef, \"spacy\")\n",
    "create_collection(\"semantic_splitter/\", spacy_ef, \"spacy\")\n",
    "\n",
    "# same for replicate\n",
    "create_collection(\"char_splitter_1024_o128/\", replicate_ef, \"replicate\")\n",
    "create_collection(\"article_regex_splitter/\", replicate_ef, \"replicate\")\n",
    "create_collection(\"semantic_splitter/\", replicate_ef, \"replicate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT FINISH, HERE ONLY TESTING FOR RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = chroma_client.get_or_create_collection(\n",
    "#     name=\"test\",\n",
    "#     embedding_function=SpacyEmbeddingsFunction(),\n",
    "#     # metadata={\"hnsw:space\": \"cosine\"} # \"l2\" (default: squared L2 norm), \"ip\" or \"cosine\"\n",
    "#     )\n",
    "\n",
    "# collection = chroma_client.get_or_create_collection(\n",
    "#     name=\"first_collection\",\n",
    "#     embedding_function=huggingface_ef,\n",
    "#     # metadata={\"hnsw:space\": \"cosine\"} # \"l2\" (default: squared L2 norm), \"ip\" or \"cosine\"\n",
    "#     )\n",
    "\n",
    "# collection.add(\n",
    "#     ids=[\"1\", \"2\", \"3\"],\n",
    "#     documents=[\"Apfel\", \"Fahrzeug\", \"Rechtssprechung\"],\n",
    "#     # metadatas=[{},{}]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb \n",
    "chroma_client = chromadb.PersistentClient(path=\"../resources/chromadb\")\n",
    "collection = chroma_client.get_collection(\"semantic_splitter_spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek(0)\n",
    "collection.count()\n",
    "# collection.modify(name=\"new_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.query(\n",
    "#     query_embeddings=[], # embedded question / part of question # HERE: PREFORMULATE ANSWER, EMBED ANSWER, RETRIEVE REAL KNOWLEDGE ?!? # needs to be the same dimension as embedded vectors in db\n",
    "#     query_texts=[\"Obst\"], # ALTERNATIVE THAN QUERYING WITH EMBEDDINGS -> CHROMA WILL AUTOMATICALLY EMBED USING EMBEDDING FUNCTION OF COLLECTION\n",
    "#     n_results=1, # number of docs to retrieve\n",
    "#     where={\"metadata_field\": \"is_equal_to_this\"}, # filter metadata\n",
    "#     where_document={\"$contains\": \"search_string\"} # filter for hard words / regexes etc.\n",
    "#     include=[\"documents\"], specify which data to return (embeddings is excluded by default)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.get( # when wanting to not query with embeddings but only retrieve by id or so\n",
    "#     # ids=[], \n",
    "#     # where={,}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # more info: https://docs.trychroma.com/usage-guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
